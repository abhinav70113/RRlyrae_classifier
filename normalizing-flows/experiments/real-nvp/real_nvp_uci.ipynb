{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.4.1\n",
      "tensorflow-probability:  0.12.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random \n",
    "\n",
    "import numpy as np\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from normalizingflows.flow_catalog import RealNVP\n",
    "from utils.train_utils import train_density_estimation, nll\n",
    "from data import dataset_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/students/winter-term-2019/project_8/rinder/uci_data/miniboone/data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data, val_data, test_data, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_preprocess_uci\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminiboone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/AdvMethDataAnalysis/term_paper/tensorflow/normalizing-flows/data/dataset_loader.py:132\u001b[0m, in \u001b[0;36mload_and_preprocess_uci\u001b[0;34m(uci_dataset, batch_size, shuffle)\u001b[0m\n\u001b[1;32m    130\u001b[0m     data \u001b[38;5;241m=\u001b[39m uci_classes\u001b[38;5;241m.\u001b[39mGAS()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m uci_dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminiboone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43muci_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMINIBOONE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m uci_dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhepmass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/uni/AdvMethDataAnalysis/term_paper/tensorflow/normalizing-flows/data/uci_classes.py:190\u001b[0m, in \u001b[0;36mMINIBOONE.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m     file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(dt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m), data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/miniboone/data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 190\u001b[0m     trn, val, tst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_normalised\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mData(trn[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mData(val[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/uni/AdvMethDataAnalysis/term_paper/tensorflow/normalizing-flows/data/uci_classes.py:242\u001b[0m, in \u001b[0;36mMINIBOONE.load_data_normalised\u001b[0;34m(self, root_path)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data_normalised\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_path):\n\u001b[0;32m--> 242\u001b[0m     data_train, data_validate, data_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((data_train, data_validate))\n\u001b[1;32m    244\u001b[0m     mu \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/uni/AdvMethDataAnalysis/term_paper/tensorflow/normalizing-flows/data/uci_classes.py:229\u001b[0m, in \u001b[0;36mMINIBOONE.load_data\u001b[0;34m(self, root_path)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_path):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# NOTE: To remember how the pre-processing was done.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# data = pd.read_csv(root_path, names=[str(x) for x in range(50)], delim_whitespace=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# data = data[:, np.array([i for i in range(data.shape[1]) if i not in features_to_remove])]\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# np.save(\"~/data/miniboone/data.npy\", data)\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     N_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    231\u001b[0m     data_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m-\u001b[39mN_test:]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.8/site-packages/numpy/lib/npyio.py:416\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    417\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/students/winter-term-2019/project_8/rinder/uci_data/miniboone/data.npy'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, val_data, test_data, _ = dataset_loader.load_and_preprocess_uci(\"miniboone\", batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_data.take(1):\n",
    "    input_shape = sample.shape[1]\n",
    "    break\n",
    "print(input_shape)\n",
    "\n",
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(input_shape, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1\n",
    "dataset = \"gas\"\n",
    "exp_number = 1\n",
    "max_epochs = 10\n",
    "layers = 4\n",
    "shape = [256, 256]\n",
    "base_lr = 1e-4\n",
    "end_lr = 1e-5\n",
    "uci_trainsizes = {\"power\": 1659917,\n",
    "                  \"gas\": 852174,\n",
    "                  \"hepmass\": 315123,\n",
    "                  \"miniboone\": 29556,\n",
    "                  \"bsds300\": 1000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bijectors = []\n",
    "\n",
    "for i in range(layers):\n",
    "    bijectors.append(tfb.BatchNormalization())\n",
    "    bijectors.append(RealNVP(input_shape=input_shape, n_hidden=shape))\n",
    "    bijectors.append(tfp.bijectors.Permute(permutation))\n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_real_nvp')\n",
    "\n",
    "flow = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    ")\n",
    "\n",
    "# number of trainable variables\n",
    "n_trainable_variables = len(flow.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)\n",
    "\n",
    "# checkpoint_directory = \"{}/tmp_{}\".format(dataset, str(hex(random.getrandbits(32))))\n",
    "\n",
    "checkpoint_directory = \"{}/tmp_{}_{}_{}_{}\".format(dataset, layers, shape[0], shape[1], exp_number)\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)  # high value to ensure that first loss < min_loss\n",
    "min_train_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)\n",
    "min_val_epoch = 0\n",
    "min_train_epoch = 0\n",
    "delta_stop = 10  # threshold for early stopping\n",
    "\n",
    "t_start = time.time()  # start time\n",
    "\n",
    "# start training\n",
    "for i in range(max_epochs):\n",
    "\n",
    "    train_data.shuffle(buffer_size=uci_trainsizes[dataset])\n",
    "    batch_train_losses = []\n",
    "    for batch in train_data:\n",
    "        batch_loss = train_density_estimation(flow, opt, batch)\n",
    "        batch_train_losses.append(batch_loss)\n",
    "\n",
    "    train_loss = tf.reduce_mean(batch_train_losses)\n",
    "\n",
    "    if i % int(1) == 0:\n",
    "        batch_val_losses = []\n",
    "        for batch in val_data:\n",
    "            batch_loss = nll(flow, batch)\n",
    "            batch_val_losses.append(batch_loss)\n",
    "        #val_loss = nll(flow, val_data)\n",
    "        val_loss = tf.reduce_mean(batch_val_losses)\n",
    "\n",
    "        global_step.append(i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"{i}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            min_train_loss = train_loss\n",
    "            min_train_epoch = i\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = i\n",
    "            checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        elif i - min_val_epoch > delta_stop:  # no decrease in min_val_loss for \"delta_stop epochs\"\n",
    "            break\n",
    "\n",
    "train_time = time.time() - t_start\n",
    "\n",
    "# load best model with min validation loss\n",
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# perform on test dataset\n",
    "t_start = time.time()\n",
    "\n",
    "test_losses = []\n",
    "for batch in test_data:\n",
    "    batch_loss = nll(flow, batch)\n",
    "    test_losses.append(batch_loss)\n",
    "\n",
    "test_loss = tf.reduce_mean(test_losses)\n",
    "\n",
    "test_time = time.time() - t_start\n",
    "\n",
    "\n",
    "# remove checkpoint\n",
    "filelist = [f for f in os.listdir(checkpoint_directory)]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(checkpoint_directory, f))\n",
    "os.removedirs(checkpoint_directory)\n",
    "\n",
    "\n",
    "print(f'Test loss: {test_loss} at epoch: {i}')\n",
    "print(f'Average test log likelihood: {-test_loss} at epoch: {i}')\n",
    "print(f'Min val loss: {min_val_loss} at epoch: {min_val_epoch}')\n",
    "print(f'Last val loss: {val_loss} at epoch: {i}')\n",
    "print(f'Min train loss: {min_train_loss} at epoch: {min_train_epoch}')\n",
    "print(f'Last train loss: {train_loss} at epoch: {i}')\n",
    "print(f'Training time: {train_time}')\n",
    "print(f'Test time: {test_time}')\n",
    "\n",
    "results = {\n",
    "    'test_loss': float(test_loss),\n",
    "    'avg_test_logll': float(-test_loss),\n",
    "    'min_val_loss': float(min_val_loss),\n",
    "    'min_val_epoch': min_val_epoch,\n",
    "    'val_loss': float(val_loss),\n",
    "    'min_train_loss': float(min_train_loss),\n",
    "    'min_train_epoch': min_train_epoch,\n",
    "    'train_loss': float(train_loss),\n",
    "    'train_time': train_time,\n",
    "    'test_time': test_time,\n",
    "    'trained_epochs': i,\n",
    "    'trainable variables': n_trainable_variables,\n",
    "    'exp_number': exp_number\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses)\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
