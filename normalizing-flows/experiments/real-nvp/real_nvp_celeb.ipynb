{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from normalizingflows.flow_catalog import RealNVP\n",
    "from utils.train_utils import train_density_estimation, nll\n",
    "from data import dataset_loader\n",
    "\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataset = \"celeb_a\"\n",
    "layers = 8\n",
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "max_epochs = int(100)\n",
    "shape = [128, 128]\n",
    "exp_number = 1\n",
    "celeb_trainsize = 202599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load celeb dataset\n",
    "celeb_dataset = tfds.load(name=\"celeb_a\", batch_size=batch_size, shuffle_files=True)\n",
    "batched_train_data = celeb_dataset[\"train\"]\n",
    "batched_val_data = celeb_dataset[\"validation\"]\n",
    "batched_test_data = celeb_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes batch size first\n",
    "sample_batch = next(iter(batched_train_data))\n",
    "# get one random image of the batch and display it\n",
    "plt.imshow(sample_batch[\"image\"][int(np.random.rand()*batch_size)])\n",
    "plt.savefig(\"celeb_a/gt_{}.png\".format(3))\n",
    "# get shapes\n",
    "celeb_shape = sample_batch[\"image\"].shape[1:]\n",
    "input_shape = celeb_shape[0] * celeb_shape[1] * celeb_shape[2]\n",
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(shape=input_shape, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bijectors = []\n",
    "alpha = 1e-3\n",
    "\n",
    "for i in range(layers):\n",
    "    bijectors.append(tfb.BatchNormalization())\n",
    "    bijectors.append(RealNVP(input_shape=input_shape, n_hidden=shape))\n",
    "    bijectors.append(tfp.bijectors.Permute(permutation))\n",
    "    \n",
    "bijectors.append(tfb.Reshape(event_shape_out=(celeb_shape),\n",
    "                                 event_shape_in=(input_shape,)))\n",
    "\n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_real_nvp')\n",
    "\n",
    "flow = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    ")\n",
    "\n",
    "# number of trainable variables\n",
    "n_trainable_variables = len(flow.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(flow, save_dir=None):\n",
    "    plt.figure()\n",
    "    data = flow.sample(1)\n",
    "    data = tf.sigmoid(data)\n",
    "    plt.imshow(data[0])\n",
    "    if save_dir is not None:\n",
    "        plt.savefig(save_dir + \".png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)\n",
    "\n",
    "checkpoint_directory = \"{}/tmp_{}\".format(dataset, str(hex(random.getrandbits(32))))\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)  # high value to ensure that first loss < min_loss\n",
    "min_train_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)\n",
    "min_val_epoch = 0\n",
    "min_train_epoch = 0\n",
    "delta_stop = 50  # threshold for early stopping\n",
    "\n",
    "t_start = time.time()  # start time\n",
    "\n",
    "# start training\n",
    "for i in range(max_epochs):\n",
    "    \n",
    "    batched_train_data.shuffle(buffer_size=celeb_trainsize, reshuffle_each_iteration=True)\n",
    "    batch_train_losses = []\n",
    "    for batch in batched_train_data:\n",
    "        batch_loss = train_density_estimation(flow, opt, dataset_loader.logit(tf.cast(batch[\"image\"], tf.float32)))\n",
    "        batch_train_losses.append(batch_loss)\n",
    "        \n",
    "    train_loss = tf.reduce_mean(batch_train_losses)\n",
    "\n",
    "    if i % int(1) == 0:\n",
    "        batch_val_losses = []\n",
    "        for batch in batched_val_data:\n",
    "            batch_loss = nll(flow, dataset_loader.logit(tf.cast(batch[\"image\"], tf.float32)))\n",
    "            batch_val_losses.append(batch_loss)\n",
    "                \n",
    "        val_loss = tf.reduce_mean(batch_val_losses)\n",
    "        \n",
    "        global_step.append(i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"{i}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "        \n",
    "        if train_loss < min_train_loss:\n",
    "            min_train_loss = train_loss\n",
    "            min_train_epoch = i\n",
    "            \n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = i\n",
    "            checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        elif i - min_val_epoch > delta_stop:  # no decrease in min_val_loss for \"delta_stop epochs\"\n",
    "            break\n",
    "\n",
    "train_time = time.time() - t_start\n",
    "\n",
    "# load best model with min validation loss\n",
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# perform on test dataset\n",
    "t_start = time.time()\n",
    "\n",
    "test_losses = []\n",
    "for batch in batched_test_data:\n",
    "    batch_loss = nll(flow, dataset_loader.logit(tf.cast(batch[\"image\"], tf.float32)))\n",
    "    test_losses.append(batch_loss)\n",
    "    \n",
    "test_loss = tf.reduce_mean(test_losses)\n",
    "\n",
    "test_time = time.time() - t_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(global_step, train_losses, label=\"train loss\")\n",
    "plt.plot(global_step, val_losses, label=\"val loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "\n",
    "save_dir = \"{}/sampling_{}_{}_{}/\".format(dataset, layers, shape[0], shape[1])\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "for j in range(n_images):\n",
    "    plt.figure()\n",
    "    data = flow.sample(1)\n",
    "    data = tf.sigmoid(data)\n",
    "    plt.imshow(data[0])\n",
    "    plt.savefig(\"{}/{}_{}_i{}.png\".format(save_dir, exp_number, min_val_epoch, j))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
