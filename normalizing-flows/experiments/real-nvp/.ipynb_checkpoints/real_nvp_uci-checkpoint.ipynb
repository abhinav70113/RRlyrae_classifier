{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from normalizingflows.flow_catalog import RealNVP\n",
    "from utils.train_utils import train_density_estimation, nll\n",
    "from data import dataset_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, val_data, test_data, _ = dataset_loader.load_and_preprocess_uci(\"miniboone\", batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_data.take(1):\n",
    "    input_shape = sample.shape[1]\n",
    "    break\n",
    "print(input_shape)\n",
    "\n",
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(input_shape, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1\n",
    "dataset = \"gas\"\n",
    "exp_number = 1\n",
    "max_epochs = 10\n",
    "layers = 4\n",
    "shape = [256, 256]\n",
    "base_lr = 1e-4\n",
    "end_lr = 1e-5\n",
    "uci_trainsizes = {\"power\": 1659917,\n",
    "                  \"gas\": 852174,\n",
    "                  \"hepmass\": 315123,\n",
    "                  \"miniboone\": 29556,\n",
    "                  \"bsds300\": 1000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bijectors = []\n",
    "\n",
    "for i in range(layers):\n",
    "    bijectors.append(tfb.BatchNormalization())\n",
    "    bijectors.append(RealNVP(input_shape=input_shape, n_hidden=shape))\n",
    "    bijectors.append(tfp.bijectors.Permute(permutation))\n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_real_nvp')\n",
    "\n",
    "flow = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    ")\n",
    "\n",
    "# number of trainable variables\n",
    "n_trainable_variables = len(flow.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)\n",
    "\n",
    "#checkpoint_directory = \"{}/tmp_{}\".format(dataset, str(hex(random.getrandbits(32))))\n",
    "\n",
    "checkpoint_directory = \"{}/tmp_{}_{}_{}_{}\".format(dataset, layers, shape[0], shape[1], exp_number)\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)  # high value to ensure that first loss < min_loss\n",
    "min_train_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)\n",
    "min_val_epoch = 0\n",
    "min_train_epoch = 0\n",
    "delta_stop = 10  # threshold for early stopping\n",
    "\n",
    "t_start = time.time()  # start time\n",
    "\n",
    "# start training\n",
    "for i in range(max_epochs):\n",
    "\n",
    "    train_data.shuffle(buffer_size=uci_trainsizes[dataset])\n",
    "    batch_train_losses = []\n",
    "    for batch in train_data:\n",
    "        batch_loss = train_density_estimation(flow, opt, batch)\n",
    "        batch_train_losses.append(batch_loss)\n",
    "\n",
    "    train_loss = tf.reduce_mean(batch_train_losses)\n",
    "\n",
    "    if i % int(1) == 0:\n",
    "        batch_val_losses = []\n",
    "        for batch in val_data:\n",
    "            batch_loss = nll(flow, batch)\n",
    "            batch_val_losses.append(batch_loss)\n",
    "        #val_loss = nll(flow, val_data)\n",
    "        val_loss = tf.reduce_mean(batch_val_losses)\n",
    "\n",
    "        global_step.append(i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"{i}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            min_train_loss = train_loss\n",
    "            min_train_epoch = i\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = i\n",
    "            checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        elif i - min_val_epoch > delta_stop:  # no decrease in min_val_loss for \"delta_stop epochs\"\n",
    "            break\n",
    "\n",
    "train_time = time.time() - t_start\n",
    "\n",
    "# load best model with min validation loss\n",
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# perform on test dataset\n",
    "t_start = time.time()\n",
    "\n",
    "test_losses = []\n",
    "for batch in test_data:\n",
    "    batch_loss = nll(flow, batch)\n",
    "    test_losses.append(batch_loss)\n",
    "\n",
    "test_loss = tf.reduce_mean(test_losses)\n",
    "\n",
    "test_time = time.time() - t_start\n",
    "\n",
    "\n",
    "# remove checkpoint\n",
    "filelist = [f for f in os.listdir(checkpoint_directory)]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(checkpoint_directory, f))\n",
    "os.removedirs(checkpoint_directory)\n",
    "\n",
    "\n",
    "print(f'Test loss: {test_loss} at epoch: {i}')\n",
    "print(f'Average test log likelihood: {-test_loss} at epoch: {i}')\n",
    "print(f'Min val loss: {min_val_loss} at epoch: {min_val_epoch}')\n",
    "print(f'Last val loss: {val_loss} at epoch: {i}')\n",
    "print(f'Min train loss: {min_train_loss} at epoch: {min_train_epoch}')\n",
    "print(f'Last train loss: {train_loss} at epoch: {i}')\n",
    "print(f'Training time: {train_time}')\n",
    "print(f'Test time: {test_time}')\n",
    "\n",
    "results = {\n",
    "    'test_loss': float(test_loss),\n",
    "    'avg_test_logll': float(-test_loss),\n",
    "    'min_val_loss': float(min_val_loss),\n",
    "    'min_val_epoch': min_val_epoch,\n",
    "    'val_loss': float(val_loss),\n",
    "    'min_train_loss': float(min_train_loss),\n",
    "    'min_train_epoch': min_train_epoch,\n",
    "    'train_loss': float(train_loss),\n",
    "    'train_time': train_time,\n",
    "    'test_time': test_time,\n",
    "    'trained_epochs': i,\n",
    "    'trainable variables': n_trainable_variables,\n",
    "    'exp_number': exp_number\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses)\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
