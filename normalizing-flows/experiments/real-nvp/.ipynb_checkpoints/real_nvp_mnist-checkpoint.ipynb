{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.0.0\n",
      "tensorflow-probability:  0.8.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from normalizingflows.flow_catalog import RealNVP\n",
    "from utils.train_utils import train_density_estimation, nll\n",
    "from data import dataset_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 2\n",
    "train_data, val_data, test_data, _ = dataset_loader.load_and_preprocess_mnist(logit_space=True, batch_size=128, shuffle=True, classes=category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (28, 28, 1)\n",
    "size = 28\n",
    "input_shape = size*size\n",
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(input_shape, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1\n",
    "dataset = \"mnist\"\n",
    "exp_number = 1\n",
    "max_epochs = 200\n",
    "layers = 5\n",
    "shape = [256, 256]\n",
    "base_lr = 1e-4\n",
    "end_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bijectors = []\n",
    "alpha = 1e-3\n",
    "\n",
    "for i in range(layers):\n",
    "    bijectors.append(tfb.BatchNormalization())\n",
    "    bijectors.append(RealNVP(input_shape=input_shape, n_hidden=shape))\n",
    "    bijectors.append(tfp.bijectors.Permute(permutation))\n",
    "    \n",
    "bijectors.append(tfb.Reshape(event_shape_out=(size, size),\n",
    "                                 event_shape_in=(size * size,)))\n",
    "\n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_real_nvp')\n",
    "\n",
    "flow = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    ")\n",
    "\n",
    "# number of trainable variables\n",
    "n_trainable_variables = len(flow.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)\n",
    "\n",
    "checkpoint_directory = \"{}/tmp_{}_{}_{}_{}_{}\".format(dataset, layers, shape[0], shape[1], exp_number, category)\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)  # high value to ensure that first loss < min_loss\n",
    "min_train_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)\n",
    "min_val_epoch = 0\n",
    "min_train_epoch = 0\n",
    "delta_stop = 50  # threshold for early stopping\n",
    "\n",
    "t_start = time.time()  # start time\n",
    "\n",
    "# start training\n",
    "for i in range(max_epochs):\n",
    "\n",
    "    train_data.shuffle(buffer_size=1000)\n",
    "    batch_train_losses = []\n",
    "    for batch in train_data:\n",
    "        batch_loss = train_density_estimation(flow, opt, batch)\n",
    "        batch_train_losses.append(batch_loss)\n",
    "\n",
    "    train_loss = tf.reduce_mean(batch_train_losses)\n",
    "\n",
    "    if i % int(1) == 0:\n",
    "        batch_val_losses = []\n",
    "        for batch in val_data:\n",
    "            batch_loss = nll(flow, batch)\n",
    "            batch_val_losses.append(batch_loss)\n",
    "\n",
    "        val_loss = tf.reduce_mean(batch_val_losses)\n",
    "\n",
    "        global_step.append(i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"{i}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            min_train_loss = train_loss\n",
    "            min_train_epoch = i\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = i\n",
    "            checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        elif i - min_val_epoch > delta_stop:  # no decrease in min_val_loss for \"delta_stop epochs\"\n",
    "            break\n",
    "\n",
    "train_time = time.time() - t_start\n",
    "\n",
    "# load best model with min validation loss\n",
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# perform on test dataset\n",
    "t_start = time.time()\n",
    "\n",
    "test_losses = []\n",
    "for batch in test_data:\n",
    "    batch_loss = nll(flow, batch)\n",
    "    test_losses.append(batch_loss)\n",
    "\n",
    "test_loss = tf.reduce_mean(test_losses)\n",
    "\n",
    "test_time = time.time() - t_start\n",
    "\n",
    "save_dir = \"{}/sampling_{}_{}_{}_{}/\".format(dataset, layers, shape[0], shape[1], category)\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "for j in range(n_images):\n",
    "    plt.figure()\n",
    "    data = flow.sample(1)\n",
    "    data = dataset_loader.inverse_logit(data)\n",
    "    data = tf.reshape(data, (1, size, size))\n",
    "    plt.imshow(data[0], cmap='gray')\n",
    "    plt.savefig(\"{}/{}_{}_i{}.png\".format(save_dir, exp_number, min_val_epoch, j))\n",
    "    plt.close()\n",
    "\n",
    "# remove checkpoint\n",
    "filelist = [f for f in os.listdir(checkpoint_directory)]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(checkpoint_directory, f))\n",
    "os.removedirs(checkpoint_directory)\n",
    "\n",
    "print(f'Test loss: {test_loss} at epoch: {i}')\n",
    "print(f'Average test log likelihood: {-test_loss} at epoch: {i}')\n",
    "print(f'Min val loss: {min_val_loss} at epoch: {min_val_epoch}')\n",
    "print(f'Last val loss: {val_loss} at epoch: {i}')\n",
    "print(f'Min train loss: {min_train_loss} at epoch: {min_train_epoch}')\n",
    "print(f'Last train loss: {train_loss} at epoch: {i}')\n",
    "print(f'Training time: {train_time}')\n",
    "print(f'Test time: {test_time}')\n",
    "\n",
    "results = {\n",
    "    'test_loss': float(test_loss),\n",
    "    'avg_test_logll': float(-test_loss),\n",
    "    'min_val_loss': float(min_val_loss),\n",
    "    'min_val_epoch': min_val_epoch,\n",
    "    'val_loss': float(val_loss),\n",
    "    'min_train_loss': float(min_train_loss),\n",
    "    'min_train_epoch': min_train_epoch,\n",
    "    'train_loss': float(train_loss),\n",
    "    'train_time': train_time,\n",
    "    'test_time': test_time,\n",
    "    'trained_epochs': i,\n",
    "    'trainable variables': n_trainable_variables,\n",
    "    'exp_number': exp_number\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
