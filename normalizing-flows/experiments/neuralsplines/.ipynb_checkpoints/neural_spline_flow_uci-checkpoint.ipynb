{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from data.plot_samples import plot_samples_2d\n",
    "from data.visu_density import plot_heatmap_2d\n",
    "from data.dataset_loader import load_and_preprocess_uci\n",
    "from normalizingflows.flow_catalog import NeuralSplineFlow\n",
    "from utils.train_utils import train_density_no_tf, train_density_estimation\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "import time\n",
    "from utils.train_utils import sanity_check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data and the related spesifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"miniboone\"\n",
    "layers = 8\n",
    "shape = [64, 64]\n",
    "delta_count = 15\n",
    "uci_trainsizes = {\"power\": 1659917,\n",
    "                 \"gas\": 852174,\n",
    "                 \"hepmass\": 315123,\n",
    "                 \"miniboone\": 29556,\n",
    "                 \"bsds300\": 1000000}\n",
    "trainsize = uci_trainsizes[dataset]\n",
    "batched_train_data, batched_val_data, batched_test_data, intervals = load_and_preprocess_uci(dataset, batch_size=32) \n",
    "sample_batch = next(iter(batched_train_data))\n",
    "input_shape = int(sample_batch.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(input_shape, tf.float32)) \n",
    "\n",
    "bijectors = []\n",
    "\n",
    "bijector_chain = []\n",
    "for i in range(layers):\n",
    "    bijector_chain.append(NeuralSplineFlow(input_dim=input_shape, d_dim=int(input_shape/2)+1, number_of_bins=8, b_interval=intervals))\n",
    "    bijector_chain.append(tfp.bijectors.Permute(permutation))\n",
    "\n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijector_chain)), name='chain_of_real_nvp')\n",
    "\n",
    "flow = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    ")\n",
    "\n",
    "# number of trainable variables\n",
    "n_trainable_variables = flow.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, train_loss: 54.0181884765625, val_loss: 53.489410400390625\n"
     ]
    }
   ],
   "source": [
    "checkpoint_directory = \"{}/tmp_{}_{}_{}\".format(dataset, layers, shape[0], base_lr)\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_lr)  # optimizer\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow)\n",
    "\n",
    "global_step = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)  # high value to ensure that first loss < min_loss\n",
    "min_train_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)\n",
    "min_val_epoch = 0\n",
    "min_train_epoch = 0\n",
    "delta = 0  # threshold for early stopping\n",
    "\n",
    "t_start = time.time()  # start time\n",
    "\n",
    "# start training\n",
    "for i in range(max_epochs):\n",
    "    for batch in batched_train_data:\n",
    "        train_loss = train_density_no_tf(flow, optimizer, batch)\n",
    "\n",
    "    if i % int(100) == 0:\n",
    "        batch_val_losses = []\n",
    "        for batch in batched_val_data:\n",
    "            batch_loss = -tf.reduce_mean(flow.log_prob(batch))\n",
    "            batch_val_losses.append(batch_loss)\n",
    "        val_loss = tf.reduce_mean(batch_val_losses)\n",
    "        global_step.append(i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"{i}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            min_train_loss = train_loss\n",
    "            min_train_epoch = i\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = i\n",
    "\n",
    "            checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        elif i - min_val_epoch > delta_count:\n",
    "            break\n",
    "\n",
    "train_time = time.time() - t_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# perform on test dataset\n",
    "t_start = time.time()\n",
    "test_losses = []\n",
    "for batch in batched_test_data:\n",
    "    batch_loss = -tf.reduce_mean(flow.log_prob(batch))\n",
    "    test_losses.append(batch_loss)\n",
    "\n",
    "test_loss = tf.reduce_mean(test_losses)\n",
    "\n",
    "        \n",
    "test_time = time.time() - t_start\n",
    "\n",
    "# save density estimation of best model\n",
    "save_dir = \"{}/{}_density_{}_{}_{}_{}_{}\".format(dataset, dataset, batch_size, layers, shape, base_lr, min_val_epoch)\n",
    "plot_heatmap_2d(flow, -4.0, 4.0, -4.0, 4.0, name=save_dir)\n",
    "\n",
    "save_dir = \"{}/{}_sampling_{}_{}_{}_{}_{}\".format(dataset, dataset, batch_size, layers, shape, base_lr, min_val_epoch)\n",
    "plot_samples_2d(flow.sample(1000), name=save_dir)\n",
    "\n",
    "# remove checkpoint\n",
    "filelist = [f for f in os.listdir(checkpoint_directory)]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(checkpoint_directory, f))\n",
    "os.removedirs(checkpoint_directory)\n",
    "\n",
    "print(f'Test loss: {test_loss} at epoch: {i}')\n",
    "print(f'Min val loss: {min_val_loss} at epoch: {min_val_epoch}')\n",
    "print(f'Last val loss: {val_loss} at epoch: {i}')\n",
    "print(f'Min train loss: {min_train_loss} at epoch: {min_train_epoch}')\n",
    "print(f'Last val loss: {train_loss} at epoch: {i}')\n",
    "print(f'Training time: {train_time}')\n",
    "print(f'Test time: {test_time}')\n",
    "\n",
    "results = {\n",
    "    'test_loss': float(test_loss),\n",
    "    'min_val_loss': float(min_val_loss),\n",
    "    'min_val_epoch': min_val_epoch,\n",
    "    'val_loss': float(val_loss),\n",
    "    'min_train_loss': float(min_train_loss),\n",
    "    'min_train_epoch': min_train_epoch,\n",
    "    'train_loss': float(train_loss),\n",
    "    'train_time': train_time,\n",
    "    'test_time': test_time,\n",
    "    'trained_epochs': i,\n",
    "    'trainable variables': n_trainable_variables,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
