{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Estimation using Mask Autoregressive Flow (MAF) on UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.0.0\n",
      "tensorflow-probability:  0.8.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import random\n",
    "\n",
    "from data.dataset_loader import load_and_preprocess_uci\n",
    "from normalizingflows.flow_catalog import Made, BatchNorm, get_trainable_variables\n",
    "from utils.train_utils import train_density_estimation, nll\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 512\n",
    "dataset = \"power\"\n",
    "layers = 4\n",
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "max_epochs = int(500)\n",
    "shape = [64, 64]\n",
    "exp_number = 1\n",
    "uci_trainsizes = {\"power\": 1659917,\n",
    "                 \"gas\": 852174,\n",
    "                 \"hepmass\": 315123,\n",
    "                 \"miniboone\": 29556,\n",
    "                 \"bsds300\": 1000000}\n",
    "trainsize = uci_trainsizes[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train_data, batched_val_data, batched_test_data = load_and_preprocess_uci(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(batched_train_data))\n",
    "input_shape = sample_batch.shape[1]\n",
    "print(input_shape)\n",
    "\n",
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(shape=input_shape, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create MAF flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized!\n"
     ]
    }
   ],
   "source": [
    "bijectors = []\n",
    "event_shape = [input_shape]\n",
    "\n",
    "# According to [Papamakarios et al. (2017)]:\n",
    "# BatchNorm between the last autoregressive layer and the base distribution, and every two autoregressive layers\n",
    "\n",
    "bijectors.append(BatchNorm(eps=10e-5, decay=0.95))\n",
    "\n",
    "for i in range(0, layers):\n",
    "\n",
    "    bijectors.append(tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn = Made(params=2, hidden_units=shape, activation=\"relu\")))\n",
    "    bijectors.append(tfb.Permute(permutation=permutation)) # Permutation improves denstiy estimation results\n",
    "    \n",
    "    # add BatchNorm every two layers\n",
    "    if (i+1) % int(2) == 0:\n",
    "        bijectors.append(BatchNorm(eps=10e-5, decay=0.95))\n",
    "        \n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_maf')\n",
    "\n",
    "\n",
    "maf = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    "    # event_shape=[event_shape]\n",
    ")\n",
    "\n",
    "# important: initialize with log_prob to initialize the moving average of the layer statistics in the batch norm layers\n",
    "maf.log_prob(sample_batch)  # initialize\n",
    "print(\"Successfully initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21588\n"
     ]
    }
   ],
   "source": [
    "n_trainable_variables = get_trainable_variables(maf)\n",
    "print(n_trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduling\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize checkpoints\n",
    "checkpoint_directory = \"{}/tmp_{}\".format(dataset, str(hex(random.getrandbits(32))))\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)  # optimizer\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, train_loss: 2.6511566638946533, val_loss: 1.676918387413025\n",
      "1, train_loss: 1.291669487953186, val_loss: 1.0746381282806396\n",
      "2, train_loss: 0.968338668346405, val_loss: 0.9203070998191833\n",
      "3, train_loss: 0.8498308658599854, val_loss: 0.8211522698402405\n",
      "4, train_loss: 0.7767125964164734, val_loss: 0.749708354473114\n",
      "5, train_loss: 0.7166681289672852, val_loss: 0.703943133354187\n",
      "6, train_loss: 0.6707810163497925, val_loss: 0.6692290902137756\n",
      "7, train_loss: 0.6355478167533875, val_loss: 0.623394250869751\n",
      "8, train_loss: 0.6012700200080872, val_loss: 0.5904493927955627\n",
      "9, train_loss: 0.5746986865997314, val_loss: 0.5953004360198975\n",
      "10, train_loss: 0.5566505789756775, val_loss: 0.5552653670310974\n",
      "11, train_loss: 0.5321587324142456, val_loss: 0.5344226360321045\n",
      "12, train_loss: 0.5176129937171936, val_loss: 0.5268900990486145\n",
      "13, train_loss: 0.5031675100326538, val_loss: 0.497269868850708\n",
      "14, train_loss: 0.4913642108440399, val_loss: 0.4924618601799011\n",
      "15, train_loss: 0.47979089617729187, val_loss: 0.4842066764831543\n",
      "16, train_loss: 0.46459880471229553, val_loss: 0.46558165550231934\n",
      "17, train_loss: 0.4572744369506836, val_loss: 0.4583907723426819\n",
      "18, train_loss: 0.4469606876373291, val_loss: 0.4629318416118622\n",
      "19, train_loss: 0.43932071328163147, val_loss: 0.4577643573284149\n",
      "20, train_loss: 0.4272743761539459, val_loss: 0.42751356959342957\n",
      "21, train_loss: 0.4203522503376007, val_loss: 0.419996976852417\n",
      "22, train_loss: 0.4146759808063507, val_loss: 0.42020994424819946\n",
      "23, train_loss: 0.40966734290122986, val_loss: 0.4186759889125824\n",
      "24, train_loss: 0.4040687680244446, val_loss: 0.40949082374572754\n",
      "25, train_loss: 0.39196667075157166, val_loss: 0.3981102705001831\n",
      "26, train_loss: 0.38927462697029114, val_loss: 0.40274596214294434\n",
      "27, train_loss: 0.3849351704120636, val_loss: 0.3917427659034729\n",
      "28, train_loss: 0.3773028254508972, val_loss: 0.3902186453342438\n",
      "29, train_loss: 0.37523600459098816, val_loss: 0.37082260847091675\n",
      "30, train_loss: 0.37240949273109436, val_loss: 0.3847058117389679\n",
      "31, train_loss: 0.3637007176876068, val_loss: 0.36812564730644226\n",
      "32, train_loss: 0.35784637928009033, val_loss: 0.3657798767089844\n",
      "33, train_loss: 0.35623008012771606, val_loss: 0.36491522192955017\n",
      "34, train_loss: 0.34750837087631226, val_loss: 0.35419782996177673\n",
      "35, train_loss: 0.34624406695365906, val_loss: 0.34701257944107056\n",
      "36, train_loss: 0.3435339033603668, val_loss: 0.350455105304718\n",
      "37, train_loss: 0.3373475670814514, val_loss: 0.346510112285614\n",
      "38, train_loss: 0.33323895931243896, val_loss: 0.3448314070701599\n",
      "39, train_loss: 0.3313738703727722, val_loss: 0.3390709161758423\n",
      "40, train_loss: 0.32771414518356323, val_loss: 0.332176148891449\n",
      "41, train_loss: 0.3193093538284302, val_loss: 0.3280188739299774\n",
      "42, train_loss: 0.32161661982536316, val_loss: 0.3486507534980774\n",
      "43, train_loss: 0.31671592593193054, val_loss: 0.3274473547935486\n",
      "44, train_loss: 0.31517964601516724, val_loss: 0.3155028820037842\n",
      "45, train_loss: 0.3100152909755707, val_loss: 0.3464083969593048\n",
      "46, train_loss: 0.3064059019088745, val_loss: 0.309945672750473\n",
      "47, train_loss: 0.3047025799751282, val_loss: 0.3037395179271698\n",
      "48, train_loss: 0.3045649826526642, val_loss: 0.3073640763759613\n",
      "49, train_loss: 0.30127444863319397, val_loss: 0.3100702166557312\n",
      "50, train_loss: 0.3002171516418457, val_loss: 0.30321913957595825\n",
      "51, train_loss: 0.2932007610797882, val_loss: 0.29714518785476685\n",
      "52, train_loss: 0.2926713824272156, val_loss: 0.2953158915042877\n",
      "53, train_loss: 0.2904689610004425, val_loss: 0.2977663576602936\n",
      "54, train_loss: 0.2855997383594513, val_loss: 0.30967581272125244\n",
      "55, train_loss: 0.2858802378177643, val_loss: 0.286722332239151\n",
      "56, train_loss: 0.2817653715610504, val_loss: 0.3016337454319\n",
      "57, train_loss: 0.27858224511146545, val_loss: 0.2890142500400543\n",
      "58, train_loss: 0.28106528520584106, val_loss: 0.28616446256637573\n",
      "59, train_loss: 0.27913376688957214, val_loss: 0.29459553956985474\n",
      "60, train_loss: 0.2751947343349457, val_loss: 0.2817707657814026\n",
      "61, train_loss: 0.2743885815143585, val_loss: 0.2762690782546997\n",
      "62, train_loss: 0.2730577886104584, val_loss: 0.2832057774066925\n",
      "63, train_loss: 0.27054455876350403, val_loss: 0.27549198269844055\n",
      "64, train_loss: 0.2672393321990967, val_loss: 0.2805427610874176\n",
      "65, train_loss: 0.2640209496021271, val_loss: 0.2741088569164276\n",
      "66, train_loss: 0.26623961329460144, val_loss: 0.2748863995075226\n",
      "67, train_loss: 0.26558390259742737, val_loss: 0.26615625619888306\n",
      "68, train_loss: 0.26138633489608765, val_loss: 0.269571453332901\n",
      "69, train_loss: 0.26017192006111145, val_loss: 0.264940083026886\n",
      "70, train_loss: 0.2609584629535675, val_loss: 0.2640441060066223\n",
      "71, train_loss: 0.25964075326919556, val_loss: 0.2662162780761719\n",
      "72, train_loss: 0.2584172785282135, val_loss: 0.26498815417289734\n",
      "73, train_loss: 0.25248655676841736, val_loss: 0.2675113379955292\n",
      "74, train_loss: 0.25323575735092163, val_loss: 0.25253915786743164\n",
      "75, train_loss: 0.2505120038986206, val_loss: 0.273383766412735\n",
      "76, train_loss: 0.2531175911426544, val_loss: 0.2562638521194458\n",
      "77, train_loss: 0.2506285607814789, val_loss: 0.26363858580589294\n",
      "78, train_loss: 0.25077518820762634, val_loss: 0.24941116571426392\n",
      "79, train_loss: 0.24652507901191711, val_loss: 0.251345694065094\n",
      "80, train_loss: 0.24626055359840393, val_loss: 0.24711880087852478\n",
      "81, train_loss: 0.2454439103603363, val_loss: 0.25421079993247986\n",
      "82, train_loss: 0.2414577156305313, val_loss: 0.24246907234191895\n",
      "83, train_loss: 0.24135427176952362, val_loss: 0.2387644350528717\n",
      "84, train_loss: 0.24132367968559265, val_loss: 0.26760098338127136\n",
      "85, train_loss: 0.23777367174625397, val_loss: 0.2459603250026703\n",
      "86, train_loss: 0.23814070224761963, val_loss: 0.24890132248401642\n",
      "87, train_loss: 0.23747305572032928, val_loss: 0.23521478474140167\n",
      "88, train_loss: 0.23771341145038605, val_loss: 0.23375973105430603\n",
      "89, train_loss: 0.23618625104427338, val_loss: 0.24291905760765076\n",
      "90, train_loss: 0.23317204415798187, val_loss: 0.2326980084180832\n",
      "91, train_loss: 0.23038853704929352, val_loss: 0.2514948844909668\n",
      "92, train_loss: 0.23111847043037415, val_loss: 0.23902231454849243\n",
      "93, train_loss: 0.2321215718984604, val_loss: 0.23845648765563965\n",
      "94, train_loss: 0.22886711359024048, val_loss: 0.23973529040813446\n",
      "95, train_loss: 0.22774770855903625, val_loss: 0.23097926378250122\n",
      "96, train_loss: 0.2288832664489746, val_loss: 0.23879563808441162\n",
      "97, train_loss: 0.22516532242298126, val_loss: 0.22757494449615479\n",
      "98, train_loss: 0.22332118451595306, val_loss: 0.2309706211090088\n",
      "99, train_loss: 0.22268271446228027, val_loss: 0.22401580214500427\n",
      "100, train_loss: 0.22453679144382477, val_loss: 0.22270967066287994\n",
      "101, train_loss: 0.22216904163360596, val_loss: 0.23281553387641907\n",
      "102, train_loss: 0.22025063633918762, val_loss: 0.23676851391792297\n",
      "103, train_loss: 0.21919262409210205, val_loss: 0.22142411768436432\n",
      "104, train_loss: 0.22003856301307678, val_loss: 0.23408745229244232\n",
      "105, train_loss: 0.2151423990726471, val_loss: 0.21704648435115814\n",
      "106, train_loss: 0.2167396992444992, val_loss: 0.22795610129833221\n",
      "107, train_loss: 0.21642157435417175, val_loss: 0.23201146721839905\n",
      "108, train_loss: 0.21787330508232117, val_loss: 0.23194536566734314\n",
      "109, train_loss: 0.2138826549053192, val_loss: 0.2183917611837387\n",
      "110, train_loss: 0.21251781284809113, val_loss: 0.2205093652009964\n",
      "111, train_loss: 0.20955230295658112, val_loss: 0.21717587113380432\n",
      "112, train_loss: 0.2098713368177414, val_loss: 0.21574287116527557\n",
      "113, train_loss: 0.21158438920974731, val_loss: 0.2315768450498581\n",
      "114, train_loss: 0.2090635597705841, val_loss: 0.22379057109355927\n",
      "115, train_loss: 0.20714396238327026, val_loss: 0.21744637191295624\n",
      "116, train_loss: 0.20862239599227905, val_loss: 0.2194509506225586\n",
      "117, train_loss: 0.20647597312927246, val_loss: 0.20730862021446228\n",
      "118, train_loss: 0.20546559989452362, val_loss: 0.2107783854007721\n",
      "119, train_loss: 0.20246148109436035, val_loss: 0.21099470555782318\n",
      "120, train_loss: 0.2045440673828125, val_loss: 0.21407639980316162\n",
      "121, train_loss: 0.20379941165447235, val_loss: 0.21915681660175323\n",
      "122, train_loss: 0.20297172665596008, val_loss: 0.20569685101509094\n",
      "123, train_loss: 0.2052817940711975, val_loss: 0.22782817482948303\n",
      "124, train_loss: 0.19886568188667297, val_loss: 0.2224782109260559\n",
      "125, train_loss: 0.19872015714645386, val_loss: 0.21241958439350128\n",
      "126, train_loss: 0.20212548971176147, val_loss: 0.2089175432920456\n",
      "127, train_loss: 0.1943286657333374, val_loss: 0.2109014242887497\n",
      "128, train_loss: 0.19832685589790344, val_loss: 0.22233924269676208\n",
      "129, train_loss: 0.19730012118816376, val_loss: 0.19896383583545685\n",
      "130, train_loss: 0.19490665197372437, val_loss: 0.2045309692621231\n",
      "131, train_loss: 0.1940474957227707, val_loss: 0.20828483998775482\n",
      "132, train_loss: 0.19345419108867645, val_loss: 0.2151615470647812\n",
      "133, train_loss: 0.19478800892829895, val_loss: 0.20104122161865234\n",
      "134, train_loss: 0.19362972676753998, val_loss: 0.19889004528522491\n",
      "135, train_loss: 0.19112993776798248, val_loss: 0.1938018798828125\n",
      "136, train_loss: 0.19205397367477417, val_loss: 0.19898319244384766\n",
      "137, train_loss: 0.18765470385551453, val_loss: 0.19491949677467346\n",
      "138, train_loss: 0.19148775935173035, val_loss: 0.19775961339473724\n",
      "139, train_loss: 0.1888832449913025, val_loss: 0.18948398530483246\n",
      "140, train_loss: 0.18911175429821014, val_loss: 0.19379819929599762\n",
      "141, train_loss: 0.18790172040462494, val_loss: 0.19640573859214783\n",
      "142, train_loss: 0.18742452561855316, val_loss: 0.20204561948776245\n",
      "143, train_loss: 0.18812556564807892, val_loss: 0.19672171771526337\n",
      "144, train_loss: 0.19023656845092773, val_loss: 0.20081570744514465\n",
      "145, train_loss: 0.18464162945747375, val_loss: 0.18365085124969482\n",
      "146, train_loss: 0.18560554087162018, val_loss: 0.19478511810302734\n",
      "147, train_loss: 0.1853490024805069, val_loss: 0.18724705278873444\n",
      "148, train_loss: 0.18587732315063477, val_loss: 0.2079848200082779\n",
      "149, train_loss: 0.18148432672023773, val_loss: 0.18594986200332642\n",
      "150, train_loss: 0.18567560613155365, val_loss: 0.19669565558433533\n",
      "151, train_loss: 0.18333938717842102, val_loss: 0.19737277925014496\n",
      "152, train_loss: 0.18160578608512878, val_loss: 0.20678241550922394\n",
      "153, train_loss: 0.18053679168224335, val_loss: 0.18583689630031586\n",
      "154, train_loss: 0.18080736696720123, val_loss: 0.18897566199302673\n",
      "155, train_loss: 0.1826721876859665, val_loss: 0.19109196960926056\n",
      "156, train_loss: 0.17791663110256195, val_loss: 0.18537773191928864\n",
      "157, train_loss: 0.17651982605457306, val_loss: 0.18381287157535553\n",
      "158, train_loss: 0.1810409128665924, val_loss: 0.18600796163082123\n",
      "159, train_loss: 0.17564931511878967, val_loss: 0.1807137429714203\n",
      "160, train_loss: 0.1775946468114853, val_loss: 0.1836586892604828\n",
      "161, train_loss: 0.17538966238498688, val_loss: 0.17891612648963928\n",
      "162, train_loss: 0.18053260445594788, val_loss: 0.17823868989944458\n",
      "163, train_loss: 0.1755608469247818, val_loss: 0.17471326887607574\n",
      "164, train_loss: 0.177948459982872, val_loss: 0.18681780993938446\n",
      "165, train_loss: 0.1742621660232544, val_loss: 0.19351010024547577\n",
      "166, train_loss: 0.1771467626094818, val_loss: 0.18284238874912262\n",
      "167, train_loss: 0.17311038076877594, val_loss: 0.17363141477108002\n",
      "168, train_loss: 0.17259889841079712, val_loss: 0.19249334931373596\n",
      "169, train_loss: 0.17549054324626923, val_loss: 0.179843008518219\n",
      "170, train_loss: 0.17475271224975586, val_loss: 0.18290533125400543\n",
      "171, train_loss: 0.16915719211101532, val_loss: 0.17390958964824677\n",
      "172, train_loss: 0.17259888350963593, val_loss: 0.18304434418678284\n",
      "173, train_loss: 0.16941314935684204, val_loss: 0.17986494302749634\n",
      "174, train_loss: 0.16852743923664093, val_loss: 0.20977115631103516\n",
      "175, train_loss: 0.17120039463043213, val_loss: 0.1853657066822052\n",
      "176, train_loss: 0.1679968237876892, val_loss: 0.1851813793182373\n",
      "177, train_loss: 0.1693209707736969, val_loss: 0.17886337637901306\n",
      "178, train_loss: 0.1666383296251297, val_loss: 0.16860023140907288\n",
      "179, train_loss: 0.16541337966918945, val_loss: 0.16741836071014404\n",
      "180, train_loss: 0.16863104701042175, val_loss: 0.1773345023393631\n",
      "181, train_loss: 0.16535043716430664, val_loss: 0.16644757986068726\n",
      "182, train_loss: 0.16762521862983704, val_loss: 0.17676599323749542\n",
      "183, train_loss: 0.16666574776172638, val_loss: 0.16822268068790436\n",
      "184, train_loss: 0.16560779511928558, val_loss: 0.1658429354429245\n",
      "185, train_loss: 0.16748052835464478, val_loss: 0.17101339995861053\n",
      "186, train_loss: 0.16214245557785034, val_loss: 0.1697135716676712\n",
      "187, train_loss: 0.16135837137699127, val_loss: 0.17132025957107544\n",
      "188, train_loss: 0.16337810456752777, val_loss: 0.1667250245809555\n",
      "189, train_loss: 0.16040784120559692, val_loss: 0.19370827078819275\n",
      "190, train_loss: 0.16282202303409576, val_loss: 0.16415880620479584\n",
      "191, train_loss: 0.16052411496639252, val_loss: 0.16888412833213806\n",
      "192, train_loss: 0.16152967512607574, val_loss: 0.16754330694675446\n",
      "193, train_loss: 0.16104400157928467, val_loss: 0.16270987689495087\n",
      "194, train_loss: 0.1607166826725006, val_loss: 0.16817829012870789\n",
      "195, train_loss: 0.15975409746170044, val_loss: 0.17832493782043457\n",
      "196, train_loss: 0.1605120301246643, val_loss: 0.16402608156204224\n",
      "197, train_loss: 0.1578431874513626, val_loss: 0.18285445868968964\n",
      "198, train_loss: 0.15855495631694794, val_loss: 0.1641896367073059\n",
      "199, train_loss: 0.15632151067256927, val_loss: 0.1666296273469925\n",
      "200, train_loss: 0.15671421587467194, val_loss: 0.16873686015605927\n",
      "201, train_loss: 0.15578438341617584, val_loss: 0.17003066837787628\n",
      "202, train_loss: 0.15688051283359528, val_loss: 0.16374310851097107\n",
      "203, train_loss: 0.15788885951042175, val_loss: 0.15709498524665833\n",
      "204, train_loss: 0.15656034648418427, val_loss: 0.16233330965042114\n",
      "205, train_loss: 0.1572263240814209, val_loss: 0.16936315596103668\n",
      "206, train_loss: 0.15609009563922882, val_loss: 0.15973587334156036\n",
      "207, train_loss: 0.15657725930213928, val_loss: 0.1568499058485031\n",
      "208, train_loss: 0.15319949388504028, val_loss: 0.1607380211353302\n",
      "209, train_loss: 0.15556733310222626, val_loss: 0.16132454574108124\n",
      "210, train_loss: 0.15293718874454498, val_loss: 0.17303983867168427\n",
      "211, train_loss: 0.15411485731601715, val_loss: 0.15932488441467285\n",
      "212, train_loss: 0.1565214991569519, val_loss: 0.1639872044324875\n",
      "213, train_loss: 0.15211957693099976, val_loss: 0.1634110063314438\n",
      "214, train_loss: 0.15248443186283112, val_loss: 0.15920625627040863\n",
      "215, train_loss: 0.15527772903442383, val_loss: 0.15754134953022003\n",
      "216, train_loss: 0.15454450249671936, val_loss: 0.16885097324848175\n",
      "217, train_loss: 0.14868788421154022, val_loss: 0.1595844328403473\n",
      "218, train_loss: 0.14972323179244995, val_loss: 0.1588972806930542\n",
      "219, train_loss: 0.15185247361660004, val_loss: 0.15875238180160522\n",
      "220, train_loss: 0.1511649489402771, val_loss: 0.1529734581708908\n",
      "221, train_loss: 0.15003541111946106, val_loss: 0.1599547564983368\n",
      "222, train_loss: 0.14700904488563538, val_loss: 0.15930740535259247\n",
      "223, train_loss: 0.1514914184808731, val_loss: 0.1600768268108368\n",
      "224, train_loss: 0.1475755274295807, val_loss: 0.1678410768508911\n",
      "225, train_loss: 0.14866754412651062, val_loss: 0.1678609848022461\n",
      "226, train_loss: 0.14981956779956818, val_loss: 0.14830350875854492\n",
      "227, train_loss: 0.14803199470043182, val_loss: 0.17749154567718506\n",
      "228, train_loss: 0.14818407595157623, val_loss: 0.16175082325935364\n",
      "229, train_loss: 0.14826896786689758, val_loss: 0.15432560443878174\n",
      "230, train_loss: 0.1448182314634323, val_loss: 0.16328206658363342\n",
      "231, train_loss: 0.14678075909614563, val_loss: 0.15703849494457245\n"
     ]
    }
   ],
   "source": [
    "global_step = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)  # high value to ensure that first loss < min_loss\n",
    "min_train_loss = tf.convert_to_tensor(np.inf, dtype=tf.float32)\n",
    "min_val_epoch = 0\n",
    "min_train_epoch = 0\n",
    "delta_stop = 25  # threshold for early stopping\n",
    "\n",
    "t_start = time.time()  # start time\n",
    "\n",
    "# start training\n",
    "for i in range(max_epochs):\n",
    "    \n",
    "    batched_train_data.shuffle(buffer_size=trainsize, reshuffle_each_iteration=True)\n",
    "    batch_train_losses = []\n",
    "    for batch in batched_train_data:\n",
    "        batch_loss = train_density_estimation(maf, opt, batch)\n",
    "        batch_train_losses.append(batch_loss)\n",
    "        \n",
    "    train_loss = tf.reduce_mean(batch_train_losses)\n",
    "\n",
    "    if i % int(1) == 0:\n",
    "        batch_val_losses = []\n",
    "        for batch in batched_val_data:\n",
    "            batch_loss = nll(maf, batch)\n",
    "            batch_val_losses.append(batch_loss)\n",
    "                \n",
    "        val_loss = tf.reduce_mean(batch_val_losses)\n",
    "        \n",
    "        global_step.append(i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"{i}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            min_train_loss = train_loss\n",
    "            min_train_epoch = i\n",
    "            \n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = i\n",
    "            checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        elif i - min_val_epoch > delta_stop:  # no decrease in min_val_loss for \"delta_stop epochs\"\n",
    "            break\n",
    "\n",
    "train_time = time.time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model with min validation loss\n",
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# perform on test dataset\n",
    "t_start = time.time()\n",
    "\n",
    "test_losses = []\n",
    "for batch in batched_test_data:\n",
    "    batch_loss = nll(maf, batch)\n",
    "    test_losses.append(batch_loss)\n",
    "    \n",
    "test_loss = tf.reduce_mean(test_losses)\n",
    "\n",
    "test_time = time.time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss curve\n",
    "plt.plot(global_step, train_losses, label=\"train loss\")\n",
    "plt.plot(global_step, val_losses, label=\"val loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
